# -*- coding: utf-8 -*-
"""ArhamSheikh_ECE1513_Assingment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17o3n9bCnhyBEFd9SioVJCtrh80Y3Yg5w
"""

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import log_loss, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

"""# 1"""

X, y = datasets.load_iris(return_X_y=True) # Loading Dataset Iris with X Features and Y Labels

# Only use the first 100 samples and first two features only
X = X[:100, :2]
y = y[:100]

print(X.shape)
print(y.shape)

#Split into 20% Training and 80% Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)

print("Training data shape: ", X_train.shape)
print("Training labels shape: ", y_train.shape)
print("Testing data shape: ", X_test.shape)
print("Testing labels shape: ", y_test.shape)

# Plot training points
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='blue', label='Class 1 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='blue', label='Class 1 (Testing)', marker='x')

plt.title("Data space")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()

# Train Logistic Regression Model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train) # Fitting the model to training data

# Calculate training and testing loss
train_loss = log_loss(y_train, log_reg.predict_proba(X_train))
test_loss = log_loss(y_test, log_reg.predict_proba(X_test))

# Calculate training and testing accuracy
train_accuracy = accuracy_score(y_train, log_reg.predict(X_train))
test_accuracy = accuracy_score(y_test, log_reg.predict(X_test))


plt.figure()
# Plot training points
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='blue', label='Class 1 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='blue', label='Class 1 (Testing)', marker='x')

# Plot decision boundary
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = log_reg.predict_proba(xy)[:, 1].reshape(XX.shape) # Predict the class for each point in the meshgrid to plot decision boundaries

# Contour plot for decision boundary
ax.contour(XX, YY, Z, levels=[0.5], colors='k', linestyles='-')

plt.title("Logistic Regression Decision Boundary")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()

"""# 2"""

print(f'Training set accuracy: {train_accuracy * 100:.2f}%')  # Output training accuracy
print(f'Test set accuracy: {test_accuracy * 100:.2f}%')  # Output test accuracy

"""#3,


"""

X, y = datasets.load_iris(return_X_y=True) # Loading Dataset Iris with X Features and Y Labels

# Only use the first 100 samples and first two features only
X = X[:100, :2]
y = y[:100]

print(X.shape)
print(y.shape)
#Split into 20% Training and 80% Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)

print("Training data shape: ", X_train.shape)
print("Training labels shape: ", y_train.shape)
print("Testing data shape: ", X_test.shape)
print("Testing labels shape: ", y_test.shape)

# Plot training points
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='blue', label='Class 1 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='blue', label='Class 1 (Testing)', marker='x')

plt.title("Data space")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()

# Init and fit the model
svm = SVC(kernel='linear', probability= True)
svm.fit(X_train, y_train)

# Calculate training and testing accuracy
train_accuracy = svm.score(X_train, y_train)
test_accuracy = svm.score(X_test, y_test)

train_loss = log_loss(y_train, svm.predict_proba(X_train))
test_loss = log_loss(y_test, svm.predict_proba(X_test))

train_accuracy = accuracy_score(y_train, svm.predict(X_train))
test_accuracy = accuracy_score(y_test, svm.predict(X_test))

# Print the accuracy
print(f'Training set accuracy: {train_accuracy * 100:.2f}%')  # Output training accuracy
print(f'Test set accuracy: {test_accuracy * 100:.2f}%')  # Output test accuracy


plt.figure()

# Plot training points
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='blue', label='Class 1 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='blue', label='Class 1 (Testing)', marker='x')

# Plot decision boundary
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create grid to evaluate model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = svm.decision_function(xy).reshape(XX.shape)

# Plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])

# Circle the support vectors
ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], facecolors='none', edgecolors='k', s=100, label='Support Vectors', linewidth=1.5)

plt.title("Linear SVM Decision Boundary")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()

"""#4"""

# Print the accuracy
print(f'Training set accuracy: {train_accuracy * 100:.2f}%')
print(f'Test set accuracy: {test_accuracy * 100:.2f}%')

"""# 5"""

# After fitting the model
w = svm.coef_[0]  # The weight vector (w)
margin = 2 / np.linalg.norm(w)  # Calculate the margin

print(f"Weight vector: {w}")
print(f"Margin: {margin}")

"""# 6,

## Which vector is orthogonal to the decision boundary?

The weight vector is othorgonal to the SVM decision boundary, we know this sicne the hyperplane for the decision boundary is found by w^T * x + b, where the w is the weight vector, x is the input vector, and b is the bias term. The weight vector defines the parameters of maximum seperation between the two classes.

# 7

## Do the binary linear classifier and SVM have the same decision boundaries?

They do not have the same decision boundaries. For a Binary Linear Classifier, the decision boundary is determined by minimizing the classification error of the algorithm, whereas in SVM, the decision boundary is based on the dot product between the weight vector and the input vector. In SVM, the hyperplane focuses on maximizing the margin between the classes and the hyperplane. On the other hand, the decision boundary in a Binary Linear Classifier is the line that best separates the two classes based on probabilities.

#8
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import log_loss, accuracy_score

# Load the Iris dataset
X, y = datasets.load_iris(return_X_y=True)

# Use only the first 100 samples and the first two features
X = X[:100, :2]
y = y[:100]

# Split into 60% Training and 40% Testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

print("Training data shape: ", X_train.shape)
print("Training labels shape: ", y_train.shape)
print("Testing data shape: ", X_test.shape)
print("Testing labels shape: ", y_test.shape)

# Plot training points
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='blue', label='Class 1 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='blue', label='Class 1 (Testing)', marker='x')

plt.title("Data space")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()

# Initialize and fit the model with a linear kernel
kernel = 'linear'  # Set the kernel type
svm = SVC(kernel=kernel, probability=True, C=1e6)
svm.fit(X_train, y_train)

# Calculate training and testing accuracy
train_accuracy = svm.score(X_train, y_train)
test_accuracy = svm.score(X_test, y_test)

train_loss = log_loss(y_train, svm.predict_proba(X_train))
test_loss = log_loss(y_test, svm.predict_proba(X_test))

train_accuracy = accuracy_score(y_train, svm.predict(X_train))
test_accuracy = accuracy_score(y_test, svm.predict(X_test))

# Print the accuracy
print(f'Training set accuracy: {train_accuracy * 100:.2f}%')
print(f'Test set accuracy: {test_accuracy * 100:.2f}%')

# Create the plot
plt.figure()

# Plot training points
plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], c='blue', label='Class 1 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c='blue', label='Class 1 (Testing)', marker='x')

# Plot decision boundary
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create grid to evaluate model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = svm.decision_function(xy).reshape(XX.shape)

# Plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])

# Circle the support vectors
ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], facecolors='none', edgecolors='k', s=100, label='Support Vectors', linewidth=1.5)

plt.title("Linear SVM Decision Boundary")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()

# After fitting the model
w = svm.coef_[0]  # The weight vector (w)
margin = 2 / np.linalg.norm(w)  # Calculate the margin

print(f"Weight vector: {w}")
print(f"Margin: {margin}")

"""##Does the decision boundary change, test accuracy?

From what I've observed, the decision boundary changed only slightly, almost negligibly. We know that the hyperplane in SVM relies on support vectors to define the margins, and since the support vectors did not change dramatically—some may have been removed, but those closest to the boundary remained—the boundary did not shift significantly. If the support vectors had moved or different ones were identified, then yes, the boundary would have changed. However, that was not the case here. Furthermore, the testing accuracy remained the same due to the decision boundary not changing.

# 9

## How can you deal with it non linear seperable data.

To deal with non-linearly separable data using SVM, we must use a kernel that maps the original feature dimension to a higher one where the data can be linearly classified. This involves raising the dimensions of the data. A kernel such as the RBF kernel, which we learned in class, can be used to achieve this, creating decision boundaries that are non-linear in the original feature space but linear in the transformed space.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the full Iris dataset (all 150 samples and the first two features)
X, y = datasets.load_iris(return_X_y=True)
X = X[:, :2]  # Only use the first two features for visualization

# Split the dataset into training and testing sets with test_size = 0.4

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

print("Training data shape: ", X_train.shape)
print("Training labels shape: ", y_train.shape)
print("Testing data shape: ", X_test.shape)
print("Testing labels shape: ", y_test.shape)

# Plot training points
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='blue', label='Class 1 (Training)', marker='o')
plt.scatter(X_train[y_train == 2, 0], X_train[y_train == 2, 1], c='green', label='Class 2 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], c='blue', label='Class 1 (Testing)', marker='x')
plt.scatter(X_test[y_test == 2, 0], X_test[y_test == 2, 1], c='green', label='Class 2 (Testing)', marker='x')

plt.title("Data space")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(loc='best')
plt.show()


# Train a non-linear SVM classifier with the RBF kernel
svm = SVC(kernel='rbf', gamma='scale', probability=True, C=1e2)  # Use specified parameters
svm.fit(X_train, y_train)

# Calculate accuracy for training and test sets
train_accuracy = svm.score(X_train, y_train)  # Get training accuracy
test_accuracy = svm.score(X_test, y_test)      # Get testing accuracy

# Print the accuracy
print(f'Training set accuracy: {train_accuracy * 100:.2f}%')
print(f'Test set accuracy: {test_accuracy * 100:.2f}%')

# Define the plot boundaries
x_min = X_train[:, 0].min() - 0.5
x_max = X_train[:, 0].max() + 0.5
y_min = X_train[:, 1].min() - 0.5
y_max = X_train[:, 1].max() + 0.5

# Create a meshgrid for plotting the decision boundaries
step = 0.02
xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))

# Predict the class for each point in the meshgrid
Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and margins
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')

# Plot training points
plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], c='red', label='Class 0 (Training)', marker='o')
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], c='blue', label='Class 1 (Training)', marker='o')
plt.scatter(X_train[y_train == 2, 0], X_train[y_train == 2, 1], c='green', label='Class 2 (Training)', marker='o')

# Plot testing points
plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c='red', label='Class 0 (Testing)', marker='x')
plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c='blue', label='Class 1 (Testing)', marker='x')
plt.scatter(X_test[y_test == 2, 0], X_test[y_test == 2, 1], c='green', label='Class 2 (Testing)', marker='x')

# Set plot labels and title
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('Non-linear SVM Decision Boundary with RBF Kernel (Test size = 0.4)')
plt.legend(loc='best')

# Show the plot
plt.show()